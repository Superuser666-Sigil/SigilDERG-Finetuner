# Phase 1: Broad Rust training
# Train on wide Rust dataset with moderate filters to get rich coverage
model_name: meta-llama/Meta-Llama-3.1-8B-Instruct

dataset:
  names:
    - ammarnasr/the-stack-rust-clean
    # Add more Rust datasets here for broader coverage
  use_cache: true
  # Moderate filtering - allow broader coverage
  min_length: 64
  max_length: 200000
  exclude_tests: true
  exclude_examples: false  # Include examples in Phase 1
  exclude_benches: true
  # Quality heuristics - optional for Phase 1
  prefer_idiomatic: false  # Don't restrict too much in Phase 1
  prefer_documented: false
  idiomatic_quality_ratio: 2.0  # Ratio of idiomatic patterns to low-quality markers required (higher = more strict)
  shuffle_seed: 42

max_seq_len: 4096  # Allow longer sequences in Phase 1
pack: true

lora:
  r: 16
  alpha: 16
  dropout: 0.05
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - up_proj
    - down_proj
    - gate_proj

train:
  micro_batch_size: 8
  gradient_accumulation: 6
  lr: 1.0e-4  # Standard learning rate for Phase 1
  weight_decay: 0.0
  num_steps: 12000  # Full training in Phase 1
  warmup_steps: 250
  logging_steps: 10  # More frequent logging for better visibility
  save_every: 1000
  bf16: true
  grad_checkpointing: true
  max_grad_norm: 1.0
  log_backend: tensorboard
  lr_scheduler_type: cosine
  optimizer: paged_adamw_8bit
  save_total_limit: 3

bnb_4bit:
  quant_type: nf4
  compute_dtype: bfloat16
  use_double_quant: true

misc:
  output_dir: out/llama8b-rust-qlora-phase1
  logging_dir: out/llama8b-rust-qlora-phase1/logs
  seed: 42

