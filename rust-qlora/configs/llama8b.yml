model_name: meta-llama/Meta-Llama-3.1-8B-Instruct

# Dataset configuration (supports multiple datasets)
# Stricter defaults for high-quality, compilable, idiomatic code
dataset:
  # Single dataset name or list of dataset names
  names:
    - ammarnasr/the-stack-rust-clean
    # Add more Rust datasets here for broader coverage
  # Optional: specify cache directory to avoid network bottlenecks
  # cache_dir: ~/.cache/huggingface/datasets
  use_cache: true
  # Filtering options - tightened for quality
  min_length: 128  # Filter out very short/noisy snippets
  max_length: 200000
  exclude_tests: true
  exclude_examples: true  # Exclude examples for Phase 2 sharpening
  exclude_benches: true
  # Quality heuristics - ENABLED for high-quality training
  prefer_idiomatic: true  # Require idiomatic Rust patterns (Result/Option/derives/traits)
  prefer_documented: true  # Require documentation comments
  # Optional: shuffle dataset (uses memory, use with caution)
  shuffle_seed: 42  # Enable shuffling for better training

# Backward compatibility
dataset_name: ammarnasr/the-stack-rust-clean

# sequence & packing
# Reduced for better focus on single-file, compilable programs
max_seq_len: 2048  # Focus on complete, self-contained programs
pack: true

# LoRA (safe starting point for 8B)
lora:
  r: 16
  alpha: 16
  dropout: 0.05
  target_modules:
    - q_proj; k_proj; v_proj; o_proj; up_proj; down_proj; gate_proj

train:
  micro_batch_size: 8           # H100 80GB can also do 12; start 8, bump if headroom
  gradient_accumulation: 6      # effective batch = 48 seqs
  lr: 1.0e-4
  weight_decay: 0.0
  num_steps: 12000              # extend to 20k if metrics keep improving
  warmup_steps: 250
  logging_steps: 10  # More frequent logging for better visibility (every ~2 min at ~12s/step)
  eval_every: 500               # how often eval loop should run (external script handles cadence)
  save_every: 1000
  bf16: true
  grad_checkpointing: true
  max_grad_norm: 1.0
  # Logging backend: "tensorboard", "wandb", or null/empty for no logging
  log_backend: tensorboard
  # Additional training options
  lr_scheduler_type: cosine      # cosine, linear, constant, etc.
  optimizer: paged_adamw_8bit    # paged_adamw_8bit, adamw_torch, etc.
  save_total_limit: 3            # Keep only last N checkpoints
  load_best_model_at_end: false  # Requires evaluation dataset

bnb_4bit:
  quant_type: nf4
  compute_dtype: bfloat16
  use_double_quant: true

misc:
  output_dir: out/llama8b-rust-qlora
  # TensorBoard logs will be saved here
  logging_dir: out/llama8b-rust-qlora/logs
  seed: 42
